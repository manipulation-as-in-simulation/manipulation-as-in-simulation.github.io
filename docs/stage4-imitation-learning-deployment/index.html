<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta content="text/html; charset=utf-8" http-equiv="Content-Type">
<title>Stage 4: Imitation Learning and Deploy in the Real World - ManipAsInSim's Sim-to-Real Tutorial</title>
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<meta name="generator" content="mkdocs-1.6.1, mkdocs-gitbook-1.0.7">
<meta name="author" content="ManipAsInSim Team">
<link rel="shortcut icon" href="../images/favicon.ico" type="image/x-icon">
<meta name="HandheldFriendly" content="true"/>
<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="black">
<meta rel="next" href="" />
<link href="../css/style.min.css" rel="stylesheet">
<link href="../assets/_mkdocstrings.css" rel="stylesheet">
<link href="../custom.css" rel="stylesheet"> 
</head>

<body>
<div class="book">
<div class="book-summary">
<div id="book-search-input" role="search">
<input type="text" placeholder="Type to search" />
</div> <!-- end of book-search-input -->

<nav role="navigation">
<ul class="summary">
<li>
<a href=".."  class="custom-link">ManipAsInSim's Sim-to-Real Tutorial</a>
</li>
<li class="divider"></li>
<li class="header"><a href="../stage1-task-scene-construction/index.html" class="">Stage 1 - Task and Scene Construction</a></li>

<li>
<a href="../stage1-task-scene-construction#overview" class="">Overview</a>
</li>

<li>
<a href="../stage1-task-scene-construction#task-example" class="">Task Example</a>
</li>

<li>
<a href="../stage1-task-scene-construction#code-reference" class="">Code Reference</a>
</li>

<li>
<a href="../stage1-task-scene-construction#next-steps" class="">Next Steps</a>
</li>

<li class="header"><a href="../stage2-calibration/index.html" class="">Stage 2 - Camera Calibration</a></li>

<li>
<a href="../stage2-calibration#overview" class="">Overview</a>
</li>

<li>
<a href="../stage2-calibration#calibration-workflow" class="">Calibration Workflow</a>
</li>

<li>
<a href="../stage2-calibration#advanced-techniques" class="">Advanced Techniques</a>
</li>

<li>
<a href="../stage2-calibration#code-reference" class="">Code Reference</a>
</li>

<li>
<a href="../stage2-calibration#calibration-data-management" class="">Calibration Data Management</a>
</li>

<li>
<a href="../stage2-calibration#next-steps" class="">Next Steps</a>
</li>

<li class="header"><a href="../stage3-data-generation/index.html" class="">Stage 3 - Data Generation</a></li>

<li>
<a href="../stage3-data-generation#overview" class="">Overview</a>
</li>

<li>
<a href="../stage3-data-generation#data-collection" class="">Data Collection</a>
</li>

<li class="subheader">
<a href="../stage3-data-generation#task-decomposition-and-annotation" class="">Task Decomposition and Annotation</a>
</li>

<li class="subitem">
<a href="../stage3-data-generation#kitchen-task-subtask" class="">Kitchen task example</a>
</li>

<li>
<a href="../stage3-data-generation#canteen-task-subtask" class="">Canteen task example</a>
</li>

<li>
<a href="../stage3-data-generation#large-scale-data-generation" class="">Large-scale Data Generation</a>
</li>

<li>
<a href="../stage3-data-generation#detail-of-wbcmimicgen" class="">Detail of WBCMimicGen</a>
</li>

<li class="subitem">
  <a href="../stage3-data-generation#data-generation-of-mobile-manipulator" class="">Data Generation of Mobile Manipulator</a>
</li>
  

<li>
<a href="../stage3-data-generation#code-reference" class="">Code Reference</a>
</li>

<li>
<a href="../stage3-data-generation#next-steps" class="">Next Steps</a>
</li>

<li class="header"><a href="../stage4-imitation-learning-deployment/index.html" class="">Stage 4 - Imitation Learning and Deployment</a></li>

<li>
<a href="../stage4-imitation-learning-deployment#overview" class="">Overview</a>
</li>

<li>
<a href="../stage4-imitation-learning-deployment#policy-training" class="">Policy Training</a>
</li>

<li class="subheader">
<a href="../stage4-imitation-learning-deployment#real-world-deployment" class="">Real-World Deployment</a>
</li>

<li class="subitem">
<a href="../stage4-imitation-learning-deployment#depth-prediction-via-cdm" class="">Depth Prediction via CDMs</a>
</li>

<li>
<a href="../stage4-imitation-learning-deployment#code-reference" class="">Code Reference</a>
</li>

<li class="divider"></li>



<li><a href="http://www.mkdocs.org">
Published with MkDocs
</a></li>

<li><a href="https://github.com/GitbookIO/theme-default">
Theme by GitBook
</a></li>
</ul>

</nav>

</div> <!-- end of book-summary -->

<div class="book-body">
<div class="body-inner">
<div class="book-header" role="navigation">

<!-- Title -->
<h1>
<i class="fa fa-circle-o-notch fa-spin"></i>
<a href="." ></a>
</h1>

</div> <!-- end of book-header -->

<div class="page-wrapper" tabindex="-1" role="main">
<div class="page-inner">
<div id="book-search-results">
<div class="search-noresults">

<section class="normal markdown-section">



<h1 id="stage-4-imitation-learning-and-deploy-in-the-real-world">Stage 4: Imitation Learning and Real-World Deployment<a class="headerlink" href="#stage-4-imitation-learning-and-deploy-in-the-real-world" title="Permanent link"></a></h1>
<h2 id="overview" style="margin: 10px 0; padding: 0px">Overview<a class="headerlink" href="#overview" title="Permanent link"></a></h2>

<div style="margin: 0px 0px 20px 0px; padding: 0px 0; background: linear-gradient(to right, #f8f9fa, #ffffff); border-radius: 12px; box-shadow: 0 4px 12px rgba(0,0,0,0.08);">
  <div style="position: relative; width: 100%; max-width: 1200px; margin: 0 auto; overflow-x: auto;">
  <img src="../images/stage-pictures/cdm-pipeline-seq-stage-4.png" alt="ManipAsInSim Pipeline Overview" style="width: 100%; height: auto; display: block; border-radius: 8px; box-shadow: 0 2px 8px rgba(0,0,0,0.1);">
  </div>
</div>

<p>This stage leverages the generated demonstration data to train visuomotor control policies via imitation learning, enabling direct transfer to physical systems. We employ a depth-based variant of the PPT (Perception-Policy-Transfer) framework<a href="#ref-1">[1,2]</a>, utilizing depth representations as the primary visual modality for enhanced sim-to-real transfer.</p>
<h2 id="policy-training">Policy Training<a class="headerlink" href="#policy-training" title="Permanent link"></a></h2>
<p>Our architecture adapts a pre-trained ResNet encoder for depth image processing (with modified single-channel input layer), combines proprioceptive state encoding via MLP, and employs a diffusion head for action sequence generation<a href="#ref-1">[1,2]</a>. The policy learns temporal action distributions from Stage 3 demonstrations, achieving robust task completion in simulation:</p>
<div class="rgb-grid-container">
  <div class="rgb-grid-item">
    <!-- <img src="../images/calib/mask/mask_0.png" alt="RGB Frame 0 - Initial Position"> -->
    <video src="../videos/sim/eval-clean-plate/rgb/camera_0.mp4" alt="RGB Frame 0 - Initial Position" controls autoplay loop muted>
  </div>
  <div class="rgb-grid-item">
    <video src="../videos/sim/eval-microwave/rgb/camera_0.mp4" alt="RGB Frame 4 - Final Position" controls autoplay loop muted>
  </div>
  <div class="rgb-grid-item">
    <video src="../videos/sim/eval-clean-plate/depth/camera_0.mp4" alt="RGB Frame 0 - Initial Position" controls autoplay loop muted>
  </div>
  <div class="rgb-grid-item">
    <video src="../videos/sim/eval-microwave/depth/camera_0.mp4" alt="RGB Frame 3 - Intermediate Position" controls autoplay loop muted>
  </div>
</div>

<h2 id="real-world-deployment">Real-World Deployment<a class="headerlink" href="#real-world-deployment" title="Permanent link"></a></h2>
<p>Deployment utilizes ManiUniCon, an asynchronous control framework that decouples perception, inference, and actuation into parallel processes, minimizing latency and ensuring smooth trajectory execution despite computational overhead.</p>
<h3 id="depth-prediction-via-cdm">Depth Prediction via CDMs<a class="headerlink" href="#depth-prediction-via-cdm" title="Permanent link"></a></h3>
<p>Physical depth sensors exhibit significant noise characteristics absent in simulation, creating a domain gap that degrades policy performance. We mitigate this through Camera Depth Models (CDMs)â€”neural architectures that synthesize high-fidelity depth maps from RGB inputs. These models leverage learned geometric priors to produce depth representations comparable to simulated quality, facilitating robust sim-to-real transfer.</p>
<p><img alt="Image #1" src="../images/00ffea.png" /></p>
<h2 id="code-reference">Code Reference<a class="headerlink" href="#code-reference" title="Permanent link"></a></h2>
<p><strong>Policy Architecture:</strong></p>
<ul>
<li>Configuration: <code><a href="https://github.com/Ericonaldo/ppt_learning/blob/main/configs/config_eval_depth_unified.yaml">https://github.com/Ericonaldo/ppt_learning/blob/main/configs/config_eval_depth_unified.yaml</a></code></li>
</ul>
<p><strong>ManiUniCon Framework:</strong></p>
<ul>
<li>Control pipeline: <code><a href="https://github.com/Universal-Control/ManiUniCon/blob/main/main.py">https://github.com/Universal-Control/ManiUniCon/blob/main/main.py</a></code></li>
<li>Hardware interface: <code><a href="https://github.com/Universal-Control/ManiUniCon/blob/main/maniunicon/robot_interface/ur5_robotiq.py">https://github.com/Universal-Control/ManiUniCon/blob/main/maniunicon/robot_interface/ur5_robotiq.py</a></code></li>
</ul>
<p><strong>CDM Integration:</strong></p>
<ul>
<li>Observation processing: <code><a href="https://github.com/Universal-Control/ManiUniCon/blob/main/maniunicon/customize/obs_wrapper/ppt_depth_wrapper_cdm.py">https://github.com/Universal-Control/ManiUniCon/blob/main/maniunicon/customize/obs_wrapper/ppt_depth_wrapper_cdm.py</a></code></li>
</ul>

<h2 id="references">References<a class="headerlink" href="#references" title="Permanent link"></a></h2>
<p id="ref-1">[1] Pu Hua, Minghuan Liu, Annabella Macaluso, Yunfeng Lin, Weinan Zhang, Huazhe Xu, and Lirui Wang. GenSim2: Scaling robot data generation with multi-modal and reasoning LLMs. arXiv preprint arXiv:2410.03645, 2024.</p>
<p id="ref-2">[2] Haotong Lin, Sida Peng, Jingxiao Chen, Songyou Peng, Jiaming Sun, Minghuan Liu, Hujun Bao, Jiashi Feng, Xiaowei Zhou, and Bingyi Kang. Prompting depth anything for 4k resolution accurate metric depth estimation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 17070â€“17080, 2025.</p>


</section>
</div> <!-- end of search-noresults -->
<div class="search-results">
<div class="has-results">

<h1 class="search-results-title"><span class='search-results-count'></span> results matching "<span class='search-query'></span>"</h1>
<ul class="search-results-list"></ul>

</div> <!-- end of has-results -->
<div class="no-results">

<h1 class="search-results-title">No results matching "<span class='search-query'></span>"</h1>

</div> <!-- end of no-results -->
</div> <!-- end of search-results -->
</div> <!-- end of book-search-results -->

</div> <!-- end of page-inner -->
</div> <!-- end of page-wrapper -->

</div> <!-- end of body-inner -->

</div> <!-- end of book-body -->
<style>
/* Hide search results by default */
#book-search-results .search-results {
    display: none;
}
#book-search-results.active .search-noresults {
    display: none;
}
#book-search-results.active .search-results {
    display: block;
}
</style>
<script>
var base_url = '..';
var min_search_length = 3;
</script>
<script src="../js/main.js"></script>
<script src="../search/main.js"></script>
<script src="../js/gitbook.min.js"></script>
<script src="../js/theme.min.js"></script>
<script src="../js/nav-hierarchy.js"></script>
</body>
</html>