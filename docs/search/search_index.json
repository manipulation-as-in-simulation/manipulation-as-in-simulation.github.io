{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"stage1-task-scene-construction/","text":"Stage 1: Task and Scene Construction \u00b6 Overview \u00b6 Scene construction is the foundation of sim-to-real transfer, where a high-fidelity simulation environment that accurately represents the real-world workspace needs creating. This involves three key steps: identifying and collecting all necessary assets (robots, objects, and environment elements), precisely measuring their spatial relationships in the real world, and systematically configuring them in the simulator with appropriate physical properties and randomization ranges. Asset Identification and Collection: Analyze and collect all physical elements involved in the task, including the robot system, work surfaces, target objects, and environmental fixtures. Assets can be sourced from simple geometric primitives, open-source libraries like PartNet-Mobility and YCB Dataset, or custom models created for specific requirements. Spatial Measurement and Positioning: Measure the precise positions and orientations of all components in the real environment using laser rangefinders or calibration boards. Document fixed positions (robot base, equipment placement), randomization ranges for dynamic objects, and operational boundaries to ensure accurate spatial relationships in simulation. Simulation Environment Setup: Configure the simulation scene based on measured data, setting up static elements at their exact positions, defining randomization ranges for dynamic objects, and assigning appropriate physical properties (mass, friction, collision models) to ensure realistic interactions during task execution. Kitchen Task Example \u00b6 Using the kitchen microwave task as an example to demonstrate the complete scene construction process. Task Description \u00b6 The robot needs to grasp a bowl, place it in the microwave, and close the microwave door. Asset Collection \u00b6 Main assets involved in the Kitchen task include: UR5 robot and its fixed base Work surface Microwave (articulated model with openable door) Bowl (graspable rigid body object) The image of assets Position Measurement \u00b6 Precise measurements in the real environment: (The image of real-world measure) Simulation Implementation \u00b6 Configure the scene in IsaacLab based on measurement results: Scene Configuration Key Points: - Use fixed relative coordinate system - Set reasonable physical parameters - Configure collision detection and grasping constraints Code Reference \u00b6 Specific scene configuration implementations can be found in the following codebases: Kitchen Task Scene Configuration: File path: manip-as-in-sim-suite/wbcmimic/source/isaaclab_mimic/isaaclab_mimic/tasks/manager_based/ur5_sim/ur5_put_bowl_in_microwave_and_close.py Contains position settings and physical property configurations for microwave, bowl, and other assets Canteen Task Scene Configuration: File path: manip-as-in-sim-suite/wbcmimic/source/isaaclab_mimic/isaaclab_mimic/tasks/manager_based/ur5_sim/ur5_clean_plate.py Demonstrates multi-object scene construction methods Next Steps \u00b6 After completing scene construction, the next stage involves camera calibration to ensure visual observations in simulation are consistent with the real environment. This is crucial for vision-based policy learning.","title":"Stage 1: Task and Scene Construction"},{"location":"stage1-task-scene-construction/#stage-1-task-and-scene-construction","text":"","title":"Stage 1: Task and Scene Construction"},{"location":"stage1-task-scene-construction/#overview","text":"Scene construction is the foundation of sim-to-real transfer, where a high-fidelity simulation environment that accurately represents the real-world workspace needs creating. This involves three key steps: identifying and collecting all necessary assets (robots, objects, and environment elements), precisely measuring their spatial relationships in the real world, and systematically configuring them in the simulator with appropriate physical properties and randomization ranges. Asset Identification and Collection: Analyze and collect all physical elements involved in the task, including the robot system, work surfaces, target objects, and environmental fixtures. Assets can be sourced from simple geometric primitives, open-source libraries like PartNet-Mobility and YCB Dataset, or custom models created for specific requirements. Spatial Measurement and Positioning: Measure the precise positions and orientations of all components in the real environment using laser rangefinders or calibration boards. Document fixed positions (robot base, equipment placement), randomization ranges for dynamic objects, and operational boundaries to ensure accurate spatial relationships in simulation. Simulation Environment Setup: Configure the simulation scene based on measured data, setting up static elements at their exact positions, defining randomization ranges for dynamic objects, and assigning appropriate physical properties (mass, friction, collision models) to ensure realistic interactions during task execution.","title":"Overview"},{"location":"stage1-task-scene-construction/#kitchen-task-example","text":"Using the kitchen microwave task as an example to demonstrate the complete scene construction process.","title":"Kitchen Task Example"},{"location":"stage1-task-scene-construction/#code-reference","text":"Specific scene configuration implementations can be found in the following codebases: Kitchen Task Scene Configuration: File path: manip-as-in-sim-suite/wbcmimic/source/isaaclab_mimic/isaaclab_mimic/tasks/manager_based/ur5_sim/ur5_put_bowl_in_microwave_and_close.py Contains position settings and physical property configurations for microwave, bowl, and other assets Canteen Task Scene Configuration: File path: manip-as-in-sim-suite/wbcmimic/source/isaaclab_mimic/isaaclab_mimic/tasks/manager_based/ur5_sim/ur5_clean_plate.py Demonstrates multi-object scene construction methods","title":"Code Reference"},{"location":"stage1-task-scene-construction/#next-steps","text":"After completing scene construction, the next stage involves camera calibration to ensure visual observations in simulation are consistent with the real environment. This is crucial for vision-based policy learning.","title":"Next Steps"},{"location":"stage2-calibration/","text":"Stage 2: Camera Calibration \u00b6 Overview \u00b6 Camera calibration ensures visual consistency between simulation and reality, which is essential for successful vision-based policy transfer. We employ an optimization-based approach that leverages differentiable rendering to automatically find camera parameters that minimize the difference between simulated and real images. The process involves collecting multi-view data pairs of the robot in various configurations, extracting robot masks using Segment Anything Model (SAM), and iteratively optimizing camera extrinsics through gradient-based methods until the rendered masks closely match the real ones. Calibration Workflow \u00b6 Step 1: Data Collection \u00b6 Collect calibration data in the real environment: Design Collection Sequence Design joint position sequences covering the workspace Collect after stabilizing at each position Record precise joint angle values Data Collection For each preset joint configuration: Control robot to reach specified position Wait for robot stabilization (~0.5 seconds) Synchronously collect: Camera images Robot joint angles Timestamps Data Quality Check Ensure images are clear without motion blur Verify joint angle recording accuracy Check time synchronization of data pairs Data Collection - RGB Camera Calibration Frames Step 2: Mask Generation \u00b6 Process real images using Segment Anything Model: Data Collection - Masks of real images Step 3: Optimization Solving \u00b6 Execute camera parameter optimization: Initialization Initial camera parameters: Position: Estimates based on rough measurements Orientation: Pointing to robot workspace center Intrinsics: Using factory parameters or calibrated values Iterative Optimization Automatically Optimization loop: Set current camera parameters in simulation Render robot masks for all joint configurations Compute L1 loss with real masks Backpropagate to calculate gradients Update camera parameters Repeat until convergence Convergence Criteria Loss function change below threshold Parameter updates approaching zero Maximum iterations reached Data Collection - Error of masks after optimization Advanced Techniques \u00b6 1. Improving Calibration Accuracy \u00b6 Increase Sampling Density : Add more sampling points in critical work areas Multi-round Iteration : Use coarse calibration results as initial values for fine calibration Code Reference \u00b6 Specific camera calibration implementations can be found at: Calibration Optimization Script: File path: ManiUniCon/tools/diff_optim_camera_params.py Contains complete differentiable optimization workflow Data Collection Tools: File path: ManiUniCon/tools/save_diff_optim_data.py Used for collecting and saving calibration data Mask Generation Tools: File path: ManiUniCon/tools/generate_sam_mask.py Automatic segmentation script integrated with SAM model Calibration Data Management \u00b6 (TODO: check here) Data Organization Structure \u00b6 calibration_data/ \u251c\u2500\u2500 raw_images/ # Raw camera images \u251c\u2500\u2500 joint_states/ # Robot joint angles \u251c\u2500\u2500 masks/ # SAM-generated masks \u251c\u2500\u2500 optimization/ # Optimization process data \u2502 \u251c\u2500\u2500 initial/ # Initial parameters \u2502 \u251c\u2500\u2500 iterations/ # Iteration history \u2502 \u2514\u2500\u2500 final/ # Final results \u2514\u2500\u2500 validation/ # Validation dataset Calibration Result Storage \u00b6 # camera_params.yaml camera_d435 : position : [ 0.35 , 0.45 , 1.2 ] orientation : [ -2.8 , 0.1 , 0.0 ] intrinsics : fx : 615.0 fy : 615.0 cx : 320.0 cy : 240.0 distortion : [ 0.0 , 0.0 , 0.0 , 0.0 , 0.0 ] calibration_error : 1.8 # pixels calibration_date : \"2024-01-15\" Next Steps \u00b6 After completing camera calibration, we obtain precise visual system parameters, laying the foundation for subsequent data generation. The next stage will utilize the calibrated simulation environment to generate large-scale training data through teleoperation and data augmentation techniques.","title":"Stage 2: Camera Calibration"},{"location":"stage2-calibration/#stage-2-camera-calibration","text":"","title":"Stage 2: Camera Calibration"},{"location":"stage2-calibration/#overview","text":"Camera calibration ensures visual consistency between simulation and reality, which is essential for successful vision-based policy transfer. We employ an optimization-based approach that leverages differentiable rendering to automatically find camera parameters that minimize the difference between simulated and real images. The process involves collecting multi-view data pairs of the robot in various configurations, extracting robot masks using Segment Anything Model (SAM), and iteratively optimizing camera extrinsics through gradient-based methods until the rendered masks closely match the real ones.","title":"Overview"},{"location":"stage2-calibration/#calibration-workflow","text":"","title":"Calibration Workflow"},{"location":"stage2-calibration/#advanced-techniques","text":"","title":"Advanced Techniques"},{"location":"stage2-calibration/#code-reference","text":"Specific camera calibration implementations can be found at: Calibration Optimization Script: File path: ManiUniCon/tools/diff_optim_camera_params.py Contains complete differentiable optimization workflow Data Collection Tools: File path: ManiUniCon/tools/save_diff_optim_data.py Used for collecting and saving calibration data Mask Generation Tools: File path: ManiUniCon/tools/generate_sam_mask.py Automatic segmentation script integrated with SAM model","title":"Code Reference"},{"location":"stage2-calibration/#calibration-data-management","text":"(TODO: check here)","title":"Calibration Data Management"},{"location":"stage2-calibration/#next-steps","text":"After completing camera calibration, we obtain precise visual system parameters, laying the foundation for subsequent data generation. The next stage will utilize the calibrated simulation environment to generate large-scale training data through teleoperation and data augmentation techniques.","title":"Next Steps"},{"location":"stage3-data-generation/","text":"Stage 3: Data Generation \u00b6 Overview \u00b6 Multiple approaches exist for generating manipulation data, including motion planning, reinforcement learning, and teleoperation. Each has its strengths, and we adopt teleoperation combined with WBCMimicGen augmentation as a balanced approach that leverages human intuition for complex contact-rich tasks while achieving the scale needed for effective learning. WBCMimicGen decomposes trajectories into object-centric subtasks, applies spatial and temporal transformations based on object relationships, and recombines them to generate diverse training scenarios while maintaining task validity and physical plausibility. Taking the canteen cleaning task as an example, from just 3-5 human demonstrations of plate manipulation, we can generate thousands of variants with different plate positions, approach angles, and cleaning sequences. Data Collection \u00b6 Task Demonstration Execute task demonstrations by tele-operating Record complete trajectories Validate data quality Data Recording Robot states (joint angles, end-effector positions) Environmental observations (RGB images, depth maps, point clouds) Action commands (control signals, gripper states) Timestamps and metadata A demo of collected data: Task Decomposition and Annotation \u00b6 Object-Centric Decomposition \u00b6 Decompose complex tasks into object-centric subtasks. The kitchen task's example is as follows: Stage 1: grasp bowl Start: Robot initial position Goal: Bowl stably grasped Stage 2: place in microwave Start: Bowl grasped Goal: Bowl placed inside microwave Stage 3: close microwave door Start: Bowl inside microwave Goal: Microwave door closed Grasp bowl Place in microwave Close microwave door \u276e \u276f Stage 1: grasp fork Start: Robot initial position Goal: Fork stably grasped Stage 2: place fork Start: Fork stably grasped Goal: Fork placed in the blanket Stage 3: grasp plate Start: Fork placed in the blanket Goal: Plate stably grasped Stage 4: Drop plate Start: Plate stably grasped Goal: Drop above the left blanket Stage 5: Place plate Start: Drop above the left blanket Goal: Plate placed in the blanket Grasp fork Place fork Grasp plate Drop plate Place plate \u276e \u276f Large-scale Data Generation \u00b6 (video) Dataset Management \u00b6 Data Format \u00b6 Using Zarr format for storage, supporting efficient parallel read/write: Dataset structure : dataset / \u251c\u2500\u2500 observations / \u2502 \u251c\u2500\u2500 rgb / # RGB images \u2502 \u251c\u2500\u2500 depth / # Depth maps \u2502 \u251c\u2500\u2500 point_cloud / # Point cloud data \u2502 \u2514\u2500\u2500 robot_state / # Robot states \u251c\u2500\u2500 actions / # Action sequences \u251c\u2500\u2500 rewards / # Reward signals (if available) \u2514\u2500\u2500 metadata / # Metadata \u251c\u2500\u2500 success / # Success flags \u251c\u2500\u2500 subtask_labels / # Subtask annotations \u2514\u2500\u2500 timestamps / # Timestamps Code Reference \u00b6 Data generation related code: Teleoperation System: SpaceMouse control: ManiUniCon/maniunicon/policies/spacemouse.py Quest VR control: ManiUniCon/maniunicon/policies/quest.py Data Generation Scripts: Generation configuration: manip-as-in-sim-suite/wbcmimic/config/mimicgen/generate_data.yaml Parallel generation: manip-as-in-sim-suite/wbcmimic/scripts/mimicgen/generate_dataset_parallel_all.py Data replay: manip-as-in-sim-suite/wbcmimic/scripts/mimicgen/replay_demos.py Data Processing Tools: Zarr data management: manip-as-in-sim-suite/wbcmimic/source/isaaclab_mimic/isaaclab_mimic/utils/datasets/ Data visualization: manip-as-in-sim-suite/wbcmimic/scripts/data_tools/draw_data_distribution.py Next Steps \u00b6 After completing large-scale data generation, we have obtained a rich training dataset. The next stage will use this data for imitation learning, training robot control policies, and ultimately deploying them in the real world for validation.","title":"Stage 3: Data Generation"},{"location":"stage3-data-generation/#stage-3-data-generation","text":"","title":"Stage 3: Data Generation"},{"location":"stage3-data-generation/#overview","text":"Multiple approaches exist for generating manipulation data, including motion planning, reinforcement learning, and teleoperation. Each has its strengths, and we adopt teleoperation combined with WBCMimicGen augmentation as a balanced approach that leverages human intuition for complex contact-rich tasks while achieving the scale needed for effective learning. WBCMimicGen decomposes trajectories into object-centric subtasks, applies spatial and temporal transformations based on object relationships, and recombines them to generate diverse training scenarios while maintaining task validity and physical plausibility. Taking the canteen cleaning task as an example, from just 3-5 human demonstrations of plate manipulation, we can generate thousands of variants with different plate positions, approach angles, and cleaning sequences.","title":"Overview"},{"location":"stage3-data-generation/#data-collection","text":"Task Demonstration Execute task demonstrations by tele-operating Record complete trajectories Validate data quality Data Recording Robot states (joint angles, end-effector positions) Environmental observations (RGB images, depth maps, point clouds) Action commands (control signals, gripper states) Timestamps and metadata A demo of collected data:","title":"Data Collection"},{"location":"stage3-data-generation/#task-decomposition-and-annotation","text":"","title":"Task Decomposition and Annotation"},{"location":"stage3-data-generation/#large-scale-data-generation","text":"(video)","title":"Large-scale Data Generation"},{"location":"stage3-data-generation/#dataset-management","text":"","title":"Dataset Management"},{"location":"stage3-data-generation/#code-reference","text":"Data generation related code: Teleoperation System: SpaceMouse control: ManiUniCon/maniunicon/policies/spacemouse.py Quest VR control: ManiUniCon/maniunicon/policies/quest.py Data Generation Scripts: Generation configuration: manip-as-in-sim-suite/wbcmimic/config/mimicgen/generate_data.yaml Parallel generation: manip-as-in-sim-suite/wbcmimic/scripts/mimicgen/generate_dataset_parallel_all.py Data replay: manip-as-in-sim-suite/wbcmimic/scripts/mimicgen/replay_demos.py Data Processing Tools: Zarr data management: manip-as-in-sim-suite/wbcmimic/source/isaaclab_mimic/isaaclab_mimic/utils/datasets/ Data visualization: manip-as-in-sim-suite/wbcmimic/scripts/data_tools/draw_data_distribution.py","title":"Code Reference"},{"location":"stage3-data-generation/#next-steps","text":"After completing large-scale data generation, we have obtained a rich training dataset. The next stage will use this data for imitation learning, training robot control policies, and ultimately deploying them in the real world for validation.","title":"Next Steps"},{"location":"stage4-imitation-learning-deployment/","text":"Stage 4: Imitation Learning and Deploy in the Real World \u00b6 Overview \u00b6 The collected data is used to train control policies through imitation learning, which can then be transferred to the real world using physical cameras and robot sensors as policy observations. In the kitchen and canteen tasks, we adopt the PPT learning framework - a policy structure similar to those used in Hua et al. and Lin et al., but using depth as input. Policy Training \u00b6 We employ a policy structure similar to the one used in [1] and [2] that encodes the depth image with a pre-trained ResNet. The first layer of the network is replaced with a 1-channel convolutional layer; the proprioceptive states are encoded by a from-scratch MLP; a diffusion head~\\citep{chi2023diffusion,ho2020denoising} is adopted to predict the action sequence. The model learns to predict action sequences from the demonstrations collected in Stage 3. After training, model can successfully finish task in simulator: Real-World Deployment \u00b6 A asynchronous control framework - ManiUniCon is adopted for less lag caused by model inference time and better smoothness. In the kitchen and canteen tasks, we use ManiUniCon for asynchronous control, which separates perception, inference, and control into different processes. Depth Sensor Denoising \u00b6 The depth sensor in the real world is quite noisy, leading to a large domain shift in observations, which significantly harms the policy's performance. For our tasks, with the help of CDMs (Conditional Diffusion Models), we can obtain accurate depth from the noisy input. This greatly improves the model's transfer to the real world. (comparison with/o maniunicon ) Code Reference \u00b6 Policy Implementation: - PPT model: ManiUniCon/maniunicon/customize/policy_model/ppt_policy_model.py - Configuration: ManiUniCon/configs/policy/ppt_rgb.yaml Asynchronous Control: - Main controller: ManiUniCon/main.py - Robot interface: ManiUniCon/maniunicon/robot_interface/ur5_robotiq.py Depth Denoising: - CDM inference: manip-as-in-sim-suite/cdm/infer.py - Depth preprocessing: ManiUniCon/maniunicon/customize/obs_wrapper/ppt_depth_wrapper_cdm.py [1] Pu Hua, Minghuan Liu, Annabella Macaluso, Yunfeng Lin, Weinan Zhang, Huazhe Xu, and Lirui Wang. Gensim2: Scaling robot data generation with multi-modal and reasoning llms. arXiv preprint arXiv:2410.03645, 2024. [2] Haotong Lin, Sida Peng, Jingxiao Chen, Songyou Peng, Jiaming Sun, Minghuan Liu, Hujun Bao, Jiashi Feng, Xiaowei Zhou, and Bingyi Kang. Prompting depth anything for 4k resolution accurate metric depth estimation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 17070\u201317080, 2025.","title":"Stage 4: Imitation Learning and Deploy in the Real World"},{"location":"stage4-imitation-learning-deployment/#stage-4-imitation-learning-and-deploy-in-the-real-world","text":"","title":"Stage 4: Imitation Learning and Deploy in the Real World"},{"location":"stage4-imitation-learning-deployment/#overview","text":"The collected data is used to train control policies through imitation learning, which can then be transferred to the real world using physical cameras and robot sensors as policy observations. In the kitchen and canteen tasks, we adopt the PPT learning framework - a policy structure similar to those used in Hua et al. and Lin et al., but using depth as input.","title":"Overview"},{"location":"stage4-imitation-learning-deployment/#policy-training","text":"We employ a policy structure similar to the one used in [1] and [2] that encodes the depth image with a pre-trained ResNet. The first layer of the network is replaced with a 1-channel convolutional layer; the proprioceptive states are encoded by a from-scratch MLP; a diffusion head~\\citep{chi2023diffusion,ho2020denoising} is adopted to predict the action sequence. The model learns to predict action sequences from the demonstrations collected in Stage 3. After training, model can successfully finish task in simulator:","title":"Policy Training"},{"location":"stage4-imitation-learning-deployment/#real-world-deployment","text":"A asynchronous control framework - ManiUniCon is adopted for less lag caused by model inference time and better smoothness. In the kitchen and canteen tasks, we use ManiUniCon for asynchronous control, which separates perception, inference, and control into different processes.","title":"Real-World Deployment"},{"location":"stage4-imitation-learning-deployment/#code-reference","text":"Policy Implementation: - PPT model: ManiUniCon/maniunicon/customize/policy_model/ppt_policy_model.py - Configuration: ManiUniCon/configs/policy/ppt_rgb.yaml Asynchronous Control: - Main controller: ManiUniCon/main.py - Robot interface: ManiUniCon/maniunicon/robot_interface/ur5_robotiq.py Depth Denoising: - CDM inference: manip-as-in-sim-suite/cdm/infer.py - Depth preprocessing: ManiUniCon/maniunicon/customize/obs_wrapper/ppt_depth_wrapper_cdm.py [1] Pu Hua, Minghuan Liu, Annabella Macaluso, Yunfeng Lin, Weinan Zhang, Huazhe Xu, and Lirui Wang. Gensim2: Scaling robot data generation with multi-modal and reasoning llms. arXiv preprint arXiv:2410.03645, 2024. [2] Haotong Lin, Sida Peng, Jingxiao Chen, Songyou Peng, Jiaming Sun, Minghuan Liu, Hujun Bao, Jiashi Feng, Xiaowei Zhou, and Bingyi Kang. Prompting depth anything for 4k resolution accurate metric depth estimation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 17070\u201317080, 2025.","title":"Code Reference"}]}